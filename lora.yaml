model: "mistralai/Mistral-7B-v0.1"
lora: true
training_args:
  output_dir: "/tmp/llm_finetuning"
  max_steps: 5000
  per_device_train_batch_size: 8
  per_device_eval_batch_size: 4
  evaluation_strategy: "steps"
  eval_steps: 1000
  logging_strategy: "steps"
  logging_steps: 100
  save_strategy: "steps"
  save_steps: 1000
  learning_rate: 0.00005
